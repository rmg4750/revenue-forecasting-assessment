{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cvYIw2D_RlGA",
        "outputId": "65906e72-36d6-4f08-cf88-9e14fc77921c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pmdarima\n",
            "  Downloading pmdarima-2.0.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl.metadata (7.8 kB)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.11/dist-packages (from pmdarima) (1.4.2)\n",
            "Requirement already satisfied: Cython!=0.29.18,!=0.29.31,>=0.29 in /usr/local/lib/python3.11/dist-packages (from pmdarima) (3.0.12)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.11/dist-packages (from pmdarima) (2.0.2)\n",
            "Requirement already satisfied: pandas>=0.19 in /usr/local/lib/python3.11/dist-packages (from pmdarima) (2.2.2)\n",
            "Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.11/dist-packages (from pmdarima) (1.6.1)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from pmdarima) (1.14.1)\n",
            "Requirement already satisfied: statsmodels>=0.13.2 in /usr/local/lib/python3.11/dist-packages (from pmdarima) (0.14.4)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.11/dist-packages (from pmdarima) (2.3.0)\n",
            "Requirement already satisfied: setuptools!=50.0.0,>=38.6.0 in /usr/local/lib/python3.11/dist-packages (from pmdarima) (75.2.0)\n",
            "Requirement already satisfied: packaging>=17.1 in /usr/local/lib/python3.11/dist-packages (from pmdarima) (24.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.19->pmdarima) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.19->pmdarima) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.19->pmdarima) (2025.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.22->pmdarima) (3.6.0)\n",
            "Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.11/dist-packages (from statsmodels>=0.13.2->pmdarima) (1.0.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=0.19->pmdarima) (1.17.0)\n",
            "Downloading pmdarima-2.0.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl (2.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pmdarima\n",
            "Successfully installed pmdarima-2.0.4\n"
          ]
        }
      ],
      "source": [
        "!pip install pmdarima"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y pmdarima numpy\n",
        "!pip install pmdarima numpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bO3Rd2dk2_NV",
        "outputId": "416dff88-006d-4942-8eab-18ef5aabe5bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: pmdarima 2.0.4\n",
            "Uninstalling pmdarima-2.0.4:\n",
            "  Successfully uninstalled pmdarima-2.0.4\n",
            "Found existing installation: numpy 2.0.2\n",
            "Uninstalling numpy-2.0.2:\n",
            "  Successfully uninstalled numpy-2.0.2\n",
            "Collecting pmdarima\n",
            "  Using cached pmdarima-2.0.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl.metadata (7.8 kB)\n",
            "Collecting numpy\n",
            "  Downloading numpy-2.2.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.11/dist-packages (from pmdarima) (1.4.2)\n",
            "Requirement already satisfied: Cython!=0.29.18,!=0.29.31,>=0.29 in /usr/local/lib/python3.11/dist-packages (from pmdarima) (3.0.12)\n",
            "Requirement already satisfied: pandas>=0.19 in /usr/local/lib/python3.11/dist-packages (from pmdarima) (2.2.2)\n",
            "Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.11/dist-packages (from pmdarima) (1.6.1)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from pmdarima) (1.14.1)\n",
            "Requirement already satisfied: statsmodels>=0.13.2 in /usr/local/lib/python3.11/dist-packages (from pmdarima) (0.14.4)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.11/dist-packages (from pmdarima) (2.3.0)\n",
            "Requirement already satisfied: setuptools!=50.0.0,>=38.6.0 in /usr/local/lib/python3.11/dist-packages (from pmdarima) (75.2.0)\n",
            "Requirement already satisfied: packaging>=17.1 in /usr/local/lib/python3.11/dist-packages (from pmdarima) (24.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.19->pmdarima) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.19->pmdarima) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.19->pmdarima) (2025.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.22->pmdarima) (3.6.0)\n",
            "Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.11/dist-packages (from statsmodels>=0.13.2->pmdarima) (1.0.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=0.19->pmdarima) (1.17.0)\n",
            "Using cached pmdarima-2.0.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl (2.2 MB)\n",
            "Downloading numpy-2.2.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m64.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, pmdarima\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.2.5 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.2.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-2.2.5 pmdarima-2.0.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EXBW4XKogymi"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import sqlite3\n",
        "from statsmodels.tsa.stattools import adfuller\n",
        "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "#import pmdarima as pm\n",
        "from prophet import Prophet\n",
        "from sklearn.metrics import mean_absolute_percentage_error, mean_squared_error\n",
        "import warnings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ohJ3k9lTT8J"
      },
      "outputs": [],
      "source": [
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p0t6GO9eg6jU"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('Revenue.xlsx - Data.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "HSadB7-KhZA0",
        "outputId": "de326b75-9c09-41cb-e400-a5e1636de5e4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  Client Key       Type                 Treatment Type       DOS       Rev\n",
              "0  AC-000836  Product 1  Initial Consult & Ancillaries  1/1/2016   2033.27\n",
              "1  AC-000836  Product 1                            IUI  1/1/2016   1563.56\n",
              "2  AC-000836  Product 1                 IVF Freeze-All  1/1/2016  11817.29\n",
              "3  AC-000836  Product 1                        Storage  1/1/2016     90.00\n",
              "4  AC-000836  Product 1          Traditional Fresh IVF  1/1/2016   2236.34"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-27b6daa4-a1f9-4bfc-b83e-b493de212903\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Client Key</th>\n",
              "      <th>Type</th>\n",
              "      <th>Treatment Type</th>\n",
              "      <th>DOS</th>\n",
              "      <th>Rev</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>AC-000836</td>\n",
              "      <td>Product 1</td>\n",
              "      <td>Initial Consult &amp; Ancillaries</td>\n",
              "      <td>1/1/2016</td>\n",
              "      <td>2033.27</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>AC-000836</td>\n",
              "      <td>Product 1</td>\n",
              "      <td>IUI</td>\n",
              "      <td>1/1/2016</td>\n",
              "      <td>1563.56</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>AC-000836</td>\n",
              "      <td>Product 1</td>\n",
              "      <td>IVF Freeze-All</td>\n",
              "      <td>1/1/2016</td>\n",
              "      <td>11817.29</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>AC-000836</td>\n",
              "      <td>Product 1</td>\n",
              "      <td>Storage</td>\n",
              "      <td>1/1/2016</td>\n",
              "      <td>90.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>AC-000836</td>\n",
              "      <td>Product 1</td>\n",
              "      <td>Traditional Fresh IVF</td>\n",
              "      <td>1/1/2016</td>\n",
              "      <td>2236.34</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-27b6daa4-a1f9-4bfc-b83e-b493de212903')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-27b6daa4-a1f9-4bfc-b83e-b493de212903 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-27b6daa4-a1f9-4bfc-b83e-b493de212903');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-22c52a4a-18d9-410c-b19d-0349eff547c5\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-22c52a4a-18d9-410c-b19d-0349eff547c5')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-22c52a4a-18d9-410c-b19d-0349eff547c5 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "data",
              "summary": "{\n  \"name\": \"data\",\n  \"rows\": 22199,\n  \"fields\": [\n    {\n      \"column\": \"Client Key\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 167,\n        \"samples\": [\n          \"AC-005727\",\n          \"AC-006112\",\n          \"AC-003698\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Type\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"Product 2\",\n          \"Product 1\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Treatment Type\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 9,\n        \"samples\": [\n          \"Donor and Surrogacy Embryology\",\n          \"IUI\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"DOS\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 64,\n        \"samples\": [\n          \"5/1/2020\",\n          \"11/1/2020\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Rev\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 18648.251702908536,\n        \"min\": 0.0,\n        \"max\": 371419.14,\n        \"num_unique_values\": 17422,\n        \"samples\": [\n          2658.42,\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "data['Rev'] = pd.to_numeric(data['Rev'].str.replace(',', '', regex=True))\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LnNbw-xV3gAk",
        "outputId": "d11676d8-5259-494a-cd3c-5796fcd0f5f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "22199"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9k1YY7V4hl1P"
      },
      "source": [
        "## Questions About The Data:\n",
        "\n",
        "*   Are there missing values in the revenue data?\n",
        "*   How many unique treatments are within each product type?\n",
        "*   What is the total timeframe for our data?\n",
        "*   How similar are the revenues for different instances of the same treatment? Does the distribution of revenue from a specific treatment significantly change over time?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8TggogdZhh1C"
      },
      "outputs": [],
      "source": [
        "missing_data = data.isnull().sum()\n",
        "print(missing_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2L9yUDMjkYCP"
      },
      "source": [
        "This tells us that there are 46 clients that have Product 1 but not Product 2. Now lets try to understand more about the treatments falling under each product."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A1DifBf2kfvr"
      },
      "outputs": [],
      "source": [
        "product_1_treatments = data[data['Type'] == 'Product 1']['Treatment Type'].unique()\n",
        "product_2_treatments = data[data['Type'] == 'Product 2']['Treatment Type'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yo2LY1MKlVCs"
      },
      "outputs": [],
      "source": [
        "print(product_1_treatments)\n",
        "print(product_2_treatments)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q9gSiJLwl7a9"
      },
      "outputs": [],
      "source": [
        "elements_not_in_array2 = np.setdiff1d(product_1_treatments, product_2_treatments)\n",
        "print(elements_not_in_array2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WzqVQVsJmDbP"
      },
      "outputs": [],
      "source": [
        "elements_not_in_array1 = np.setdiff1d(product_2_treatments, product_1_treatments)\n",
        "print(elements_not_in_array1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZD_U8ggmKNT"
      },
      "source": [
        "We find that the only difference between products is that Product 2 does not include a Storage treatment like Product 1 does.\n",
        "\n",
        "For the following two blocks, I was curious as to the timeframe of our data and what each individual timestep was. I found the timeframe to be January 2016 to April 2021 and each datapoint represents a revenue value for the month that it happened in. Thus, our timestep is monthly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VrP2y36nma5_"
      },
      "outputs": [],
      "source": [
        "data['DOS'] = pd.to_datetime(data['DOS'])\n",
        "start_date = data['DOS'].min()\n",
        "end_date = data['DOS'].max()\n",
        "print(start_date, end_date)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vY8XlpHif6wX"
      },
      "outputs": [],
      "source": [
        "unique_dos_by_product = data.groupby('Type')['DOS'].unique()\n",
        "\n",
        "for product_type, dos_list in unique_dos_by_product.items():\n",
        "  print(f\"Product Type: {product_type}\")\n",
        "  for dos in dos_list:\n",
        "    print(dos)\n",
        "  print(\"-\" * 20)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZnp99Du7c9y"
      },
      "source": [
        "# Sample SQL Queries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yPfcdIf2w6bA"
      },
      "outputs": [],
      "source": [
        "conn = sqlite3.connect(\":memory:\")\n",
        "data.to_sql(\"data\", conn, index=False, if_exists=\"replace\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6t4h2QIwhSr"
      },
      "source": [
        "### Annual Revenue Growth by Year for Both Products"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mWhvcQ_03Ivn"
      },
      "outputs": [],
      "source": [
        "query = \"\"\"\n",
        "WITH YearlyRevenue AS (\n",
        "    SELECT\n",
        "        strftime('%Y', DOS) AS Year,\n",
        "        Type,\n",
        "        SUM(Rev) AS TotalRevenue\n",
        "    FROM data\n",
        "    GROUP BY Year, Type\n",
        "),\n",
        "Product1RevenueGrowth AS (\n",
        "    SELECT\n",
        "        Year,\n",
        "        TotalRevenue,\n",
        "        TotalRevenue - LAG(TotalRevenue, 1, 0) OVER (ORDER BY Year) AS `Product 1 Revenue Growth`\n",
        "    FROM YearlyRevenue\n",
        "    WHERE Type = 'Product 1'\n",
        ")\n",
        "SELECT * FROM Product1RevenueGrowth;\n",
        "\"\"\"\n",
        "\n",
        "df_result = pd.read_sql_query(query, conn)\n",
        "df_result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZiK-Wim3Kens"
      },
      "outputs": [],
      "source": [
        "query = \"\"\"\n",
        "WITH YearlyRevenue AS (\n",
        "    SELECT\n",
        "        strftime('%Y', DOS) AS Year,\n",
        "        Type,\n",
        "        SUM(Rev) AS TotalRevenue\n",
        "    FROM data\n",
        "    GROUP BY Year, Type\n",
        "),\n",
        "Product2RevenueGrowth AS (\n",
        "    SELECT\n",
        "        Year,\n",
        "        TotalRevenue,\n",
        "        TotalRevenue - LAG(TotalRevenue, 1, 0) OVER (ORDER BY Year) AS `Product 2 Revenue Growth`\n",
        "    FROM YearlyRevenue\n",
        "    WHERE Type = 'Product 2'\n",
        ")\n",
        "SELECT * FROM Product2RevenueGrowth;\n",
        "\"\"\"\n",
        "\n",
        "df_result = pd.read_sql_query(query, conn)\n",
        "df_result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNzhePwcLl7V"
      },
      "source": [
        "##### *Important to note that our data ends in April 2021 so the revenue growth for 2021 is inaccurate as the full years worth of data is not  available."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "701JrwZ9LSYv"
      },
      "source": [
        "### New Sales vs Returning Revenue Breakdown"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FdR8jcjB3he0"
      },
      "outputs": [],
      "source": [
        "query = \"\"\"\n",
        "WITH FirstPurchase AS (\n",
        "    SELECT \"Client Key\", MIN(DOS) AS first_purchase_date\n",
        "    FROM data\n",
        "    GROUP BY \"Client Key\"\n",
        ")\n",
        "SELECT\n",
        "    strftime('%Y', d.DOS) AS year,\n",
        "    SUM(CASE WHEN d.DOS = fp.first_purchase_date AND d.Type = 'Product 1' THEN d.Rev ELSE 0 END) AS product_1_new_sales_revenue,\n",
        "    SUM(CASE WHEN d.DOS = fp.first_purchase_date AND d.Type = 'Product 2' THEN d.Rev ELSE 0 END) AS product_2_new_sales_revenue,\n",
        "    SUM(CASE WHEN d.DOS > fp.first_purchase_date AND d.Type = 'Product 1' THEN d.Rev ELSE 0 END) AS product_1_returning_revenue,\n",
        "    SUM(CASE WHEN d.DOS > fp.first_purchase_date AND d.Type = 'Product 2' THEN d.Rev ELSE 0 END) AS product_2_returning_revenue\n",
        "FROM data d\n",
        "LEFT JOIN FirstPurchase fp ON d.\"Client Key\" = fp.\"Client Key\"\n",
        "GROUP BY year\n",
        "ORDER BY year;\n",
        "\"\"\"\n",
        "\n",
        "df_result = pd.read_sql_query(query, conn)\n",
        "df_result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y4YfKQEh8an1"
      },
      "source": [
        "## Analysis of Findings from SQL Queries\n",
        "\n",
        "\n",
        "*   Annual revenue was increasing fast before 2020. This is almost certainly due to the COVID-19 pandemic's effects on the economy\n",
        "*   A significant majority of the revenue is returning. Indicating that the service is solid enough for customers to continue buying the products.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLKjtu3qOWQM"
      },
      "source": [
        "# Forecasting Revenue for both products until the end of 2021."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o0Gb0X4FH1UD"
      },
      "source": [
        "Plot cumulative revenue - it appears to grow exponentially.\n",
        "\n",
        "I wanted to look at this to get a sense of the trend in the time series. Take a look at the hitch around March 2020."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r_RXBK6bNwv8"
      },
      "outputs": [],
      "source": [
        "revenue_by_date = data.groupby('DOS')['Rev'].sum()\n",
        "cumulative_revenue = revenue_by_date.cumsum()\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(cumulative_revenue.index, cumulative_revenue.values)\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Cumulative Revenue')\n",
        "plt.title('Cumulative Revenue over Time')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IGjh8zz693qq"
      },
      "source": [
        "Now let's look at the monthly revenue for both products combined. We can see that there is a solid trend upward with some seasonality as well. Subtle increases at the beginning of the years until around December there is negative seasonality until revenue increases again in Janaury, usually offsetting the late year decline. This is likely due to the products being health plans and customers buying their yearly plan in January. There is likely an incentive to this as we see less revenue consistently toward the end of the year."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YqyhDpqkRy7A"
      },
      "outputs": [],
      "source": [
        "monthly_revenue = data.groupby('DOS')['Rev'].sum()\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(monthly_revenue.index, monthly_revenue.values)\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Monthly Revenue')\n",
        "plt.title('Monthly Revenue over Time')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGRP8G91_BrH"
      },
      "source": [
        "Now we will look at the products separately. We see a lot of the same trends that we identified above in the individual time series but at different scales. This is because Product 2 appears to have been launched 2 years exactly after Product 1 and they both have almost the same components."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upkmHJ88Mgsr"
      },
      "outputs": [],
      "source": [
        "monthly_revenue_by_product = data.groupby(['DOS', 'Type'])['Rev'].sum().reset_index()\n",
        "monthly_revenue_product_1 = monthly_revenue_by_product[monthly_revenue_by_product['Type'] == 'Product 1']\n",
        "monthly_revenue_product_2 = monthly_revenue_by_product[monthly_revenue_by_product['Type'] == 'Product 2']\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(monthly_revenue_product_1['DOS'], monthly_revenue_product_1['Rev'])\n",
        "plt.plot(monthly_revenue_product_2['DOS'], monthly_revenue_product_2['Rev'])\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Monthly Revenue by Product')\n",
        "plt.title('Monthly Revenue by Product over Time')\n",
        "plt.legend(['Product 1', 'Product 2'])\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAlY2m0X_c7q"
      },
      "source": [
        "I decided to apply a log-transform to monthly revenue in order to offset some of the sharp upward trend. This may also help with stationarity when we are looking to fit a model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GbfIPKqVUNbD"
      },
      "outputs": [],
      "source": [
        "monthly_revenue_by_product = data.groupby(['DOS', 'Type'])['Rev'].sum().reset_index()\n",
        "monthly_revenue_product_1 = monthly_revenue_by_product[monthly_revenue_by_product['Type'] == 'Product 1']\n",
        "monthly_revenue_product_2 = monthly_revenue_by_product[monthly_revenue_by_product['Type'] == 'Product 2']\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(monthly_revenue_product_1['DOS'], np.log(monthly_revenue_product_1['Rev']))\n",
        "plt.plot(monthly_revenue_product_2['DOS'], np.log(monthly_revenue_product_2['Rev']))\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Log Monthly Revenue by Product')\n",
        "plt.title('Log Monthly Revenue by Product over Time')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exBU0X0c_wPo"
      },
      "source": [
        "Now we will begin to test for stationarity with the Augmented Dickey-Fuller (ADF) Test. Stationarity is the main assumption of ARIMA family models like the one we will build and without it our forecasts will be less reliable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_7AOsSLMM5L9"
      },
      "outputs": [],
      "source": [
        "from statsmodels.tsa.stattools import adfuller\n",
        "\n",
        "result = adfuller(monthly_revenue_product_1['Rev'])\n",
        "adf_stat, p_value, usedlag, nobs, critical_values, icbest = result\n",
        "\n",
        "print(f'ADF Statistic: {adf_stat}')\n",
        "print(f'p-value: {p_value}')\n",
        "print('Critical Values:')\n",
        "for key, value in critical_values.items():\n",
        "    print(f'   {key}, {value}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMJQ84WBIPFH"
      },
      "source": [
        "This result from ADF indicates that the raw time series for Product 1 is not stationary with p-value = 0.99.\n",
        "\n",
        "Let's apply the log-transform and see if we achieve stationarity with that."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vyBwInykAjgJ"
      },
      "outputs": [],
      "source": [
        "monthly_revenue_product_1['Log_Rev'] = np.log(monthly_revenue_product_1['Rev'])\n",
        "monthly_revenue_product_2['Log_Rev'] = np.log(monthly_revenue_product_2['Rev'])\n",
        "\n",
        "result = adfuller(monthly_revenue_product_1['Log_Rev'])\n",
        "adf_stat, p_value, usedlag, nobs, critical_values, icbest = result\n",
        "\n",
        "print(f'ADF Statistic: {adf_stat}')\n",
        "print(f'p-value: {p_value}')\n",
        "print('Critical Values:')\n",
        "for key, value in critical_values.items():\n",
        "    print(f'   {key}, {value}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JUfznfZA35O"
      },
      "source": [
        "The time series is still not stationary with p-value = 0.7.\n",
        "\n",
        "We can also apply differencing to try and achieve stationarity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W_1_2hYpRuZu"
      },
      "outputs": [],
      "source": [
        "monthly_revenue_product_1['Log_Rev_Diff'] = monthly_revenue_product_1['Log_Rev'].diff()\n",
        "monthly_revenue_product_2['Log_Rev_Diff'] = monthly_revenue_product_2['Log_Rev'].diff()\n",
        "\n",
        "monthly_revenue_product_1 = monthly_revenue_product_1.dropna()\n",
        "monthly_revenue_product_2 = monthly_revenue_product_2.dropna()\n",
        "\n",
        "result = adfuller(monthly_revenue_product_1['Log_Rev_Diff'])\n",
        "adf_stat, p_value, usedlag, nobs, critical_values, icbest = result\n",
        "\n",
        "print(f'ADF Statistic: {adf_stat}')\n",
        "print(f'p-value: {p_value}')\n",
        "print('Critical Values:')\n",
        "for key, value in critical_values.items():\n",
        "    print(f'   {key}, {value}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b3swvJojSNFp"
      },
      "outputs": [],
      "source": [
        "result = adfuller(monthly_revenue_product_2['Log_Rev_Diff'], autolag='AIC')\n",
        "adf_stat, p_value, usedlag, nobs, critical_values, icbest = result\n",
        "\n",
        "print(f'ADF Statistic: {adf_stat}')\n",
        "print(f'p-value: {p_value}')\n",
        "print('Critical Values:')\n",
        "for key, value in critical_values.items():\n",
        "    print(f'   {key}, {value}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CPivtgovIU70"
      },
      "source": [
        "This result from ADF test indicates that the differenced log-transform of the raw time series is indeed stationary. We can now proceed with our SARIMAX modeling."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UWiDIHggBYj0"
      },
      "source": [
        "I wanted to use an exogenous economic indicator in the model so I downloaded data from FRED on GDP. This GDP data is quarterly so I had to forward fill values in order to align with the monthly nature of our time series. Hopefully the inclusion of GDP will soften the impact of the COVID-19 pandemic on our data as GDP fell during this time too."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y0o-eEhmes97"
      },
      "outputs": [],
      "source": [
        "import pandas_datareader.data as web\n",
        "gdp_p1 = web.DataReader('GDP', 'fred', \"2016-01-01\", \"2021-04-01\")\n",
        "gdp_p1_monthly = gdp_p1.resample('MS').last()\n",
        "gdp_p1_monthly['GDP'] = gdp_p1_monthly['GDP'].ffill()\n",
        "gdp_p1_monthly.plot()\n",
        "\n",
        "gdp_p2 = web.DataReader('GDP', 'fred', \"2018-01-01\", \"2021-04-01\")\n",
        "gdp_p2_monthly = gdp_p2.resample('MS').last()\n",
        "gdp_p2_monthly['GDP'] = gdp_p2_monthly['GDP'].ffill()\n",
        "gdp_p2_monthly.plot()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FV7aTB8JgcAH"
      },
      "outputs": [],
      "source": [
        "gdp_p1_monthly = gdp_p1_monthly[:-1]\n",
        "gdp_p2_monthly = gdp_p2_monthly[:-1]\n",
        "\n",
        "gdp_p1_monthly.index = monthly_revenue_product_1['DOS']\n",
        "gdp_p2_monthly.index = monthly_revenue_product_2['DOS']\n",
        "\n",
        "monthly_revenue_product_1['GDP'] = gdp_p1_monthly.values\n",
        "monthly_revenue_product_2['GDP'] = gdp_p2_monthly.values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iFB3Hqw4CRFK"
      },
      "source": [
        "I am also going to include a COVID-19 indicator that is equal to zero at every timestep except for those that are outliers due to COVID-19. This will hopefully tell the model that these are not trends that we would expect to reoccur."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WAlR8fiDvNmU"
      },
      "outputs": [],
      "source": [
        "monthly_revenue_product_1['COVID'] = 0\n",
        "monthly_revenue_product_2['COVID'] = 0\n",
        "\n",
        "covid_start = '2020-03-01'\n",
        "covid_end = '2020-06-01'\n",
        "\n",
        "monthly_revenue_product_1.loc[(monthly_revenue_product_1['DOS'] >= covid_start) & (monthly_revenue_product_1['DOS'] <= covid_end), 'COVID'] = 1\n",
        "monthly_revenue_product_2.loc[(monthly_revenue_product_2['DOS'] >= covid_start) & (monthly_revenue_product_2['DOS'] <= covid_end), 'COVID'] = 1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Dt8ihQbCrFQ"
      },
      "source": [
        "Now for model building. We will fit a SARIMAX model (Seasonal Autoregressive Integrated Moving Average) to try and model revenue. In order to tune the hyperparameters we will use an auto ARIMA search which tests different sets of hyperparameters until it finds the model with the lowest AIC. AIC is a metric that tells you how well the model fits the data it was trained on while penalizing models with more parameters. This is a great metric since other goodness of fit metrics like R-squared can be inflated simply by making your model more complex. This is known as overfitting when your model isn't actually modeling the underlying relatonship and instead is modeling the random noise in your data.\n",
        "\n",
        "Once the best parameters are found, we will calculate our forecasts on the out of sample test data to evaluate performance. I am using Mean Absolute Percentage Error (MAPE) for this purpose. MAPE tells us how much our forecasts were off by as a percentage, on average. I like MAPE in this context because it is very intuitive and we are predicting large numbers so Mean Squared Error may be a bit confusing as it will also be a very large number.\n",
        "\n",
        "One quick note to avoid confusion: I modeled with log revenue instead of the differenced log revenue whiich we found to be stationary. The reason for this is that the SARIMAX model differences the data for us by setting d=1, indicating a first differencing of our data. Auto ARIMA found this to be optimal for both models as well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "nM79G_TwWlXw"
      },
      "outputs": [],
      "source": [
        "monthly_revenue_product_1['Log_Rev'] = np.log(monthly_revenue_product_1['Rev'])\n",
        "monthly_revenue_product_2['Log_Rev'] = np.log(monthly_revenue_product_2['Rev'])\n",
        "\n",
        "train_p1 = monthly_revenue_product_1[:-8]['Log_Rev']\n",
        "test_p1 = monthly_revenue_product_1[-8:]['Log_Rev']\n",
        "train_p2 = monthly_revenue_product_2[:-8]['Log_Rev']\n",
        "test_p2 = monthly_revenue_product_2[-8:]['Log_Rev']\n",
        "\n",
        "exog_train_p1 = monthly_revenue_product_1[:-8][['GDP', 'COVID']]\n",
        "exog_test_p1 = monthly_revenue_product_1[-8:][['GDP', 'COVID']]\n",
        "exog_train_p2 = monthly_revenue_product_2[:-8][['GDP', 'COVID']]\n",
        "exog_test_p2 = monthly_revenue_product_2[-8:][['GDP', 'COVID']]\n",
        "\n",
        "exog_train_p1.index = train_p1.index\n",
        "exog_train_p2.index = train_p2.index\n",
        "exog_test_p1.index = test_p1.index\n",
        "exog_test_p2.index = test_p2.index\n",
        "\n",
        "auto_p1 = pm.auto_arima(train_p1, exogenous= exog_train_p1, seasonal=True, m=12, stepwise=True, suppress_warnings=True)\n",
        "auto_p2 = pm.auto_arima(train_p2, exogenous= exog_train_p2, seasonal=True, m=12, stepwise=True, suppress_warnings=True)\n",
        "\n",
        "print(f\"\\nBest SARIMA Order for Product 1: {auto_p1.order}, Seasonal: {auto_p1.seasonal_order}\")\n",
        "print(f\"Best SARIMA Order for Product 2: {auto_p2.order}, Seasonal: {auto_p2.seasonal_order}\")\n",
        "\n",
        "model_p1 = SARIMAX(train_p1, order=auto_p1.order, seasonal_order=auto_p1.seasonal_order, exog=exog_train_p1, enforce_stationarity=True, enforce_invertibility=True)\n",
        "model_p2 = SARIMAX(train_p2, order=auto_p2.order, seasonal_order=auto_p2.seasonal_order, exog=exog_train_p2, enforce_stationarity=True, enforce_invertibility=True)\n",
        "\n",
        "results_p1 = model_p1.fit()\n",
        "results_p2 = model_p2.fit()\n",
        "\n",
        "forecast_p1 = results_p1.get_forecast(steps=len(test_p1), exog=exog_test_p1)\n",
        "forecast_p2 = results_p2.get_forecast(steps=len(test_p2), exog=exog_test_p2)\n",
        "\n",
        "preds_p1 = np.exp(forecast_p1.predicted_mean)\n",
        "ci_p1 = np.exp(forecast_p1.conf_int())\n",
        "preds_p2 = np.exp(forecast_p2.predicted_mean)\n",
        "ci_p2 = np.exp(forecast_p2.conf_int())\n",
        "\n",
        "mape_p1 = mean_absolute_percentage_error(np.exp(test_p1), preds_p1)\n",
        "mape_p2 = mean_absolute_percentage_error(np.exp(test_p2), preds_p2)\n",
        "print(f\"\\nMAPE Product 1: {mape_p1:.2f}, MAPE Product 2: {mape_p2:.2f}\")\n",
        "\n",
        "\n",
        "residuals_p1 = results_p1.resid\n",
        "residuals_p2 = results_p2.resid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E2zEb6HmGufM"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12,4))\n",
        "plt.subplot(1,2,1)\n",
        "plt.hist(residuals_p1, bins=200, alpha=0.7, color='blue')\n",
        "plt.xlim([-0.5, 0.5])\n",
        "plt.ylim([0, 20])\n",
        "plt.title(\"Residuals Histogram - Product 1\")\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.hist(residuals_p2, bins=80, alpha=0.7, color='green')\n",
        "plt.xlim([-0.5, 0.5])\n",
        "plt.ylim([0, 20])\n",
        "plt.title(\"Residuals Histogram - Product 2\")\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "moxMlQRFFVL4"
      },
      "source": [
        "Once our model is fit and predictions are made, we can analyze the residuals. They appear to be fairly normally distributed around zero with a few outliers that are not shown here as I have zoomed in on where most of the data lies.\n",
        "\n",
        "Since we are using GDP as an exogenous variable, we must estimate it for the forecasting range as well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "pu54d6xrkw42"
      },
      "outputs": [],
      "source": [
        "train_gdp_p1 = monthly_revenue_product_1['GDP']\n",
        "auto_gdp_p1 = pm.auto_arima(train_gdp_p1, seasonal=True, m=12, stepwise=True, suppress_warnings=True)\n",
        "model_gdp_p1 = SARIMAX(train_gdp_p1, order=auto_gdp_p1.order, seasonal_order=auto_gdp_p1.seasonal_order, enforce_stationarity=False, enforce_invertibility=False)\n",
        "results_gdp_p1 = model_gdp_p1.fit()\n",
        "\n",
        "forecast_gdp_p1 = results_gdp_p1.get_forecast(steps=len(test_p1))\n",
        "preds_gdp_p1 = np.exp(forecast_gdp_p1.predicted_mean)\n",
        "\n",
        "train_gdp_p2 = monthly_revenue_product_2['GDP']\n",
        "auto_gdp_p2 = pm.auto_arima(train_gdp_p2, seasonal=True, m=12, stepwise=True, suppress_warnings=True)\n",
        "model_gdp_p2 = SARIMAX(train_gdp_p2, order=auto_gdp_p2.order, seasonal_order=auto_gdp_p2.seasonal_order, enforce_stationarity=False, enforce_invertibility=False)\n",
        "results_gdp_p2 = model_gdp_p2.fit()\n",
        "\n",
        "forecast_gdp_p2 = results_gdp_p2.get_forecast(steps=len(test_p2))\n",
        "preds_gdp_p2 = np.exp(forecast_gdp_p2.predicted_mean)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "mD3a834dmA33"
      },
      "outputs": [],
      "source": [
        "forecast_gdp_new_dates = pd.date_range(start='2021-05-01', end='2021-12-01', freq='MS')\n",
        "\n",
        "new_forecast_gdp_p1 = results_gdp_p1.get_forecast(steps=len(forecast_gdp_new_dates))\n",
        "gdp_p1_forecasts = new_forecast_gdp_p1.predicted_mean\n",
        "gdp_p1_forecasts = pd.Series(gdp_p1_forecasts.values, index=forecast_gdp_new_dates, name = 'GDP')\n",
        "\n",
        "new_forecast_gdp_p2 = results_gdp_p2.get_forecast(steps=len(forecast_gdp_new_dates))\n",
        "gdp_p2_forecasts = new_forecast_gdp_p2.predicted_mean\n",
        "gdp_p2_forecasts = pd.Series(gdp_p2_forecasts.values, index=forecast_gdp_new_dates, name = 'GDP')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KNUZB1AlXi9N"
      },
      "outputs": [],
      "source": [
        "forecast_p1_test_dates = pd.date_range(start='2020-09-01', end='2021-04-01', freq='MS')\n",
        "forecast_p2_test_dates = pd.date_range(start='2020-09-01', end='2021-04-01', freq='MS')\n",
        "\n",
        "test_forecast_p1 = pd.Series(preds_p1.values, index=forecast_p1_test_dates, name = 'Rev')\n",
        "test_forecast_p2 = pd.Series(preds_p2.values, index=forecast_p2_test_dates, name = 'Rev')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jKk7TG1LZCcD"
      },
      "outputs": [],
      "source": [
        "forecast_p1_new_dates = pd.date_range(start='2021-05-01', end='2021-12-01', freq='MS')\n",
        "forecast_p2_new_dates = pd.date_range(start='2021-05-01', end='2021-12-01', freq='MS')\n",
        "\n",
        "covid_indicator = pd.Series(0, index=gdp_p1_forecasts.index, name='COVID')\n",
        "exog_p1 = pd.concat([gdp_p1_forecasts, covid_indicator], axis=1)\n",
        "exog_p2 = pd.concat([gdp_p2_forecasts, covid_indicator], axis=1)\n",
        "exog_p1 = exog_p1.astype(float).reset_index(drop=True)\n",
        "exog_p2 = exog_p2.astype(float).reset_index(drop=True)\n",
        "\n",
        "new_forecast_p1 = results_p1.get_forecast(steps=len(forecast_p1_new_dates), exog=exog_p1.values)\n",
        "new_forecast_p2 = results_p2.get_forecast(steps=len(forecast_p2_new_dates), exog=exog_p2.values)\n",
        "\n",
        "new_preds_p1 = np.exp(new_forecast_p1.predicted_mean)\n",
        "new_ci_p1 = np.exp(new_forecast_p1.conf_int())\n",
        "new_preds_p2 = np.exp(new_forecast_p2.predicted_mean)\n",
        "new_ci_p2 = np.exp(new_forecast_p2.conf_int())\n",
        "\n",
        "new_forecast_p1 = pd.Series(new_preds_p1.values, index=forecast_p1_new_dates, name = 'Rev')\n",
        "new_forecast_p2 = pd.Series(new_preds_p2.values, index=forecast_p2_new_dates, name = 'Rev')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QK-VTTwZHfAg"
      },
      "source": [
        "Now we will plot the actual time series, the test data predictions of our model, and the forecast for the rest of 2021."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mtnet1dyb0So"
      },
      "outputs": [],
      "source": [
        "plt.plot(monthly_revenue_product_1['DOS'], monthly_revenue_product_1['Rev'], label='Actual Data')\n",
        "plt.plot(forecast_p1_test_dates, test_forecast_p1, label = 'Test Forecast')\n",
        "plt.plot(new_forecast_p1, label = 'Remaining Forecast')\n",
        "plt.fill_between(new_forecast_p1.index, new_ci_p1['lower Log_Rev'].values, new_ci_p1['upper Log_Rev'].values, color='blue', alpha=0.15)\n",
        "plt.ylim([0, 12000000])\n",
        "plt.xlabel(\"Monthly Revenue\")\n",
        "plt.ylabel(\"Date\")\n",
        "plt.title(\"Product 1 Monthly Revenue + Remainder of 2021 Forecast\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ruz8c3tAHy3y"
      },
      "outputs": [],
      "source": [
        "plt.plot(monthly_revenue_product_2['DOS'], monthly_revenue_product_2['Rev'], label='Actual Data')\n",
        "plt.plot(forecast_p2_test_dates, test_forecast_p2, label = 'Test Forecast')\n",
        "plt.plot(new_forecast_p2, label = 'Remaining Forecast')\n",
        "plt.fill_between(new_forecast_p2.index, new_ci_p2['lower Log_Rev'].values, new_ci_p2['upper Log_Rev'].values, color='blue', alpha=0.15)\n",
        "plt.ylim([0, 4000000])\n",
        "plt.xlabel(\"Monthly Revenue\")\n",
        "plt.ylabel(\"Date\")\n",
        "plt.title(\"Product 2 Monthly Revenue + Remainder of 2021 Forecast\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ja_xHGzAIpGL"
      },
      "source": [
        "We can see that the test predictions are actually alright for both models. Product 2's model I would even consider using for future forecasting as it seems like the rresults it produces are completely reasonable. However, Product 1's forecast is incredibly erratic and very uncertain. For this reason I chose to employ a stronger time series model to try and get a better forecast. SARIMAX models are fantastic in that they are very interpretable but this simplicity makes it harder to forecast with them as you have to really know a lot about your data in order to model the correct relationship.\n",
        "\n",
        "Below are the final SARIMAX model summaries for both Products. You can see that we achieved 18% MAPE for Product 1 and 14% for Product 2 on the test data. These are both solid scores for MAPE."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GdcEXSWvTNBd"
      },
      "outputs": [],
      "source": [
        "print(f\"\\nMAPE Product 1: {mape_p1:.2f}, MAPE Product 2: {mape_p2:.2f}\\n\")\n",
        "print(results_p1.summary())\n",
        "print(results_p2.summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPmBV_9-KA4a"
      },
      "source": [
        "Now we will model monthly revenue for both products with Meta's Prophet Time Series model. It is much more complex than SARIMAX and you don't even need your data to be stationary. That's why we are using raw revenue instead of log revenue or differenced log revenue. Powerful models like Prophet are able to identify complex relationships in your data that you may not even be able to see yourself. That is why I employ it here. We will fit the model with several sets of parameters and choose the model with the lowest MAPE so long as it comes with a reasonable forecast and reasonable uncertainty in its predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "id": "hnTBm0da9hn1",
        "outputId": "7fab065e-1ff4-4489-bfb3-2b3820ff2a5e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameters: {'changepoint_prior_scale': 0.05, 'seasonality_prior_scale': 10, 'n_changepoints': 25, 'seasonality_mode': 'additive'}\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'monthly_revenue_product_1' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-3bb6d194b933>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparam_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Parameters:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0mforecast_with_prophet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonthly_revenue_product_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Product 1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'changepoint_prior_scale'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'seasonality_prior_scale'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'n_changepoints'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'seasonality_mode'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparam_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'monthly_revenue_product_1' is not defined"
          ]
        }
      ],
      "source": [
        "def forecast_with_prophet(df, product_name, changepoint_prior_scale, seasonality_prior_scale, n_changepoints, seasonality_mode):\n",
        "    df = df.rename(columns={'DOS': 'ds', 'Rev': 'y'})\n",
        "    train_data = df[df['ds'] < '2021-01-01']\n",
        "    test_data = df[(df['ds'] >= '2021-01-01') & (df['ds'] <= '2021-04-01')]\n",
        "\n",
        "    model = Prophet(changepoint_prior_scale=changepoint_prior_scale, seasonality_prior_scale=seasonality_prior_scale, n_changepoints=n_changepoints, seasonality_mode=seasonality_mode)\n",
        "    model.fit(train_data)\n",
        "\n",
        "    future_dates = pd.DataFrame({'ds': pd.date_range(start=train_data['ds'].min(), end='2021-12-01', freq='MS')})\n",
        "    forecast = model.predict(future_dates)\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(12, 6))\n",
        "\n",
        "    ax.plot(df['ds'], df['y'], label='Actual Data', color='blue')\n",
        "\n",
        "    ax.plot(forecast['ds'], forecast['yhat'], label='Forecast', color='red')\n",
        "    ax.fill_between(forecast['ds'], forecast['yhat_lower'], forecast['yhat_upper'], color='red', alpha=0.3, label='Confidence Interval')\n",
        "\n",
        "    ax.plot(test_data['ds'], forecast['yhat'][len(train_data):len(train_data)+len(test_data)], label='Test Set Predictions', color='green', linestyle='--')\n",
        "\n",
        "    ax.set_title(f\"Prophet Forecast for {product_name}\")\n",
        "    ax.set_xlabel(\"Date\")\n",
        "    ax.set_ylabel(\"Revenue\")\n",
        "    ax.legend()\n",
        "    plt.show()\n",
        "\n",
        "    forecasted_revenue_test = forecast['yhat'][len(train_data):len(train_data)+len(test_data)].values\n",
        "    actual_revenue_test = test_data['y'].values\n",
        "    mape = mean_absolute_percentage_error(actual_revenue_test, forecasted_revenue_test)\n",
        "    mse = mean_squared_error(actual_revenue_test, forecasted_revenue_test)\n",
        "    print(f\"Prophet Forecast for {product_name}:\")\n",
        "    print(f\"Test Set MAPE: {mape:.2f}\")\n",
        "    print(f\"Test Set MSE: {mse:.2f}\")\n",
        "\n",
        "    return forecast\n",
        "\n",
        "param_list = [\n",
        "    {'changepoint_prior_scale': 0.05, 'seasonality_prior_scale': 10, 'n_changepoints': 25, 'seasonality_mode': 'additive'},\n",
        "    {'changepoint_prior_scale': 0.01, 'seasonality_prior_scale': 10, 'n_changepoints': 25, 'seasonality_mode': 'additive'},\n",
        "    {'changepoint_prior_scale': 0.1, 'seasonality_prior_scale': 10, 'n_changepoints': 25, 'seasonality_mode': 'additive'},\n",
        "    {'changepoint_prior_scale': 0.05, 'seasonality_prior_scale': 10, 'n_changepoints': 10, 'seasonality_mode': 'additive'},\n",
        "    {'changepoint_prior_scale': 0.05, 'seasonality_prior_scale': 10, 'n_changepoints': 25, 'seasonality_mode': 'multiplicative'},\n",
        "    {'changepoint_prior_scale': 0.05, 'seasonality_prior_scale': 15, 'n_changepoints': 25, 'seasonality_mode': 'additive'},\n",
        "    {'changepoint_prior_scale': 0.01, 'seasonality_prior_scale': 15, 'n_changepoints': 10, 'seasonality_mode': 'multiplicative'}\n",
        "]\n",
        "\n",
        "for params in param_list:\n",
        "    print(\"Parameters:\", params)\n",
        "    forecast_with_prophet(monthly_revenue_product_1, \"Product 1\", params['changepoint_prior_scale'], params['seasonality_prior_scale'], params['n_changepoints'], params['seasonality_mode'])\n",
        "\n",
        "for params in param_list:\n",
        "    print(\"Parameters:\", params)\n",
        "    forecast_with_prophet(monthly_revenue_product_2, \"Product 2\", params['changepoint_prior_scale'], params['seasonality_prior_scale'], params['n_changepoints'], params['seasonality_mode'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iB31Y_oRMn4W"
      },
      "source": [
        "I have chosen the following two models for forecasting. It turns out that the hyperparameters that produced the best MAPE on test set were the same for both products."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iKnjGkxeMnM0"
      },
      "outputs": [],
      "source": [
        "forecast_product_1 = forecast_with_prophet(monthly_revenue_product_1, \"Product 1\", 0.05, 10, 10, 'additive')\n",
        "forecast_product_2 = forecast_with_prophet(monthly_revenue_product_2, \"Product 2\", 0.05, 10, 10, 'additive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uE3Y4K-6ONQ6"
      },
      "outputs": [],
      "source": [
        "forecast_product_1 = forecast_product_1.set_index('ds')\n",
        "forecast_product_2 = forecast_product_2.set_index('ds')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SLShKlQxOh5u"
      },
      "outputs": [],
      "source": [
        "preds_p1 = forecast_product_1['yhat'].iloc[-8:].values\n",
        "preds_p2 = forecast_product_2['yhat'].iloc[-8:].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ff4LhBP-EBqP"
      },
      "outputs": [],
      "source": [
        "log_monthly_revenue_product_1 = np.log(monthly_revenue_product_1['Rev'])\n",
        "log_monthly_revenue_product_2 = np.log(monthly_revenue_product_2['Rev'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SpeNfjRSXBDu"
      },
      "outputs": [],
      "source": [
        "yearly_revenue_product_1 = monthly_revenue_product_1.groupby(monthly_revenue_product_1['DOS'].dt.year)['Rev'].sum()\n",
        "yearly_revenue_product_2 = monthly_revenue_product_2.groupby(monthly_revenue_product_2['DOS'].dt.year)['Rev'].sum()\n",
        "\n",
        "print(\"Product 1 Forecasted Total Revenue for 2021: \", np.sum(preds_p1) + yearly_revenue_product_1.values[-1])\n",
        "print(\"Product 2 Forecasted Total Revenue for 2021: \", np.sum(preds_p2) + yearly_revenue_product_2.values[-1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMQs68mhKQm_"
      },
      "source": [
        "#### Cumulative Revenue with forecasts added in at the end\n",
        "\n",
        "I like the simplicity of this plot as it tells us that our Prophet forecast is in line with the trend we see in cumulative revenue."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4w18LfpI1yZd"
      },
      "outputs": [],
      "source": [
        "revenue_by_date = data.groupby('DOS')['Rev'].sum()\n",
        "preds_series = pd.Series(preds_p1 + preds_p2, index=pd.date_range(start='2021-05-01', end='2021-12-01', freq='MS'))\n",
        "total_revenue = pd.concat([revenue_by_date, preds_series])\n",
        "cumulative_revenue = total_revenue.cumsum()\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(cumulative_revenue.index, cumulative_revenue.values)\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Cumulative Revenue')\n",
        "plt.title('Cumulative Revenue over Time')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0U8ocxr0smRF"
      },
      "source": [
        "# Reflection on Modeling Task\n",
        "\n",
        "When it came to the time series forecasting step I wanted to build a SARIMAX model for simplicity. I first attempted to get my data stationary with a log transform. This did not pass ther Augmented Dickey-Fuller Test for stationarity. Nest step was to take a first difference of the revenue data and this ended up being stationary for both products. I then went on to use autocorrelation and partial autocorrelation to understand more about the lags of the revenue data. I had decided that this model would be better if it included an AR(1) component and ended up removing the MA(1) component due to numerical instability of the parameter estimates due to multicollinearity. Through several tries with different hyperparameters I could not seem to find a good fit for both products. I ended up adding GDP as an exogenous predictor and it did seem to help. Another issue I encountered was the COVID-19 pandemic's impact on revenue. It seemed as if the model had learned this as a seasonal trend so I accounted for that by interpolating over it and eventually even removing those outlying months. Neither of these solutions seemed to work. My solution for the COVID outliers was to add an indicator variable for the months with outlying values. This seemed to help but I still could not get a great fit for either of the models even after I began hyperparameter stepwise searching. I concluded that the data may be too complex to model with SARIMAX. I did end up achieving 18% MAPE and 14% MAPE on Product 1 and Product 2's out of sample test data, respectively. My decision to move forward with another model was based on the forecasting I saw after that was very erratic from Product 1's model.\n",
        "\n",
        "To overcome this complexity, I chose to forecast instead with Meta's Prophet Time Series model. This was very simple to use and ended up producing reasonable forecasts for the remainder of 2021. I researched a bit about the hyperparameters and then set up a list of different combinations to try for each of the Product's models. In the end I chose the model with the most reasonable uncertainty bounds and the lowest Mean Absolute Percent Error (MAPE) which ended up being the same set of hyperparameters for each model.\n",
        "\n",
        "Overall I would like to spend more time analyzing the data in order to find a good fit for a more intuitive time series model like SARIMAX. I learned that the benefits of a simple model used to forecast complex data are only earned through mastery of the data you're working with. More powerful models are great and very helpful but they do not require the same skill to achieve favorable results. I still really enjoyed my attempt to model this data with SARIMAX and am honestly thrilled with the results for the SARIMAX model of Product 2."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}